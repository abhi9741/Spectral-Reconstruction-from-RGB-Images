{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "random.seed(97)\n",
    "import scipy.io as io \n",
    "import numpy as np \n",
    "import cv2 \n",
    "import datetime\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(path): #why do we need this\n",
    "    ''' \n",
    "    check if the given directory exists, else create one\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def visualise_hyperspectral_cube(sample_file,channels=[],save_imgs=True,show_imgs=False):\n",
    "    '''\n",
    "    To visualise & save reflectances corresponding to each wavelength as a grayscale image\n",
    "    '''\n",
    "    sample_dump = io.loadmat(sample_file)\n",
    "    img_hsi = sample_dump['cube'] \n",
    "    img_bands = sample_dump['bands'][0] \n",
    "    img_256 = (img_hsi*255).astype(np.uint8)\n",
    "    for chnl in channels:\n",
    "        img_chnl = img_256[:,:,chnl]\n",
    "        chnl_band = img_bands[chnl-1]\n",
    "        img_save_path = sample_file.replace(\".mat\",f\"_{chnl}.png\")\n",
    "        if save_imgs:\n",
    "            cv2.imwrite(img_save_path, img_chnl)\n",
    "        if show_imgs:\n",
    "            plt.imshow(img_chnl,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise_hyperspectral_cube(\"sample/ARAD_HS_0464.mat\",[1,2,3,14],save_imgs=False,show_imgs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(net, init_type = 'xavier', init_gain = 0.02):\n",
    "    \n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
    "            if init_type == 'normal':\n",
    "                torch.nn.init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                torch.nn.init.xavier_normal_(m.weight.data, gain = init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                torch.nn.init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                torch.nn.init.orthogonal_(m.weight.data, gain = init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    # print('initialising network with %s type' % init_type)\n",
    "    net.apply(init_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state_dicts):\n",
    "    torch.save(state_dicts,CHECKPOINT_FOLDER+CHECKPOINT_FILE)\n",
    "\n",
    "def load_checkpoint(checkpoint_file,model,optimiser,learning_rate):\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimiser.load_state_dict(checkpoint[\"optimiser\"])\n",
    "\n",
    "    for params in optimiser.param_groups:\n",
    "        params[\"lr\"] = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.range(start = 0, end = 31).reshape([1, 8, 2, 2])\n",
    "# y = F.pixel_shuffle(x, 2)\n",
    "# x_ = pixel_unshuffle(y, 2)\n",
    "# print(f\"-{x.shape},y-{y.shape},x_{x_.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSI_Dataset_Train(Dataset):\n",
    "    def __init__(self,train_data_dir,input_image_shape,data_transforms=None):\n",
    "        self.train_data_dir = train_data_dir\n",
    "        self.data_transforms = data_transforms\n",
    "        self.rgb_path = self.train_data_dir+\"rgb/\"\n",
    "        self.hsi_path = self.train_data_dir + \"hsi/\"\n",
    "        self.input_image_shape = input_image_shape\n",
    "        self.input_image_size = self.input_image_shape[0]\n",
    "        \n",
    "        #Get file names\n",
    "        self.img_root_names = []\n",
    "        for img_name in os.listdir(self.hsi_path):\n",
    "            self.img_root_names.append(img_name.split(\".mat\")[0])\n",
    "\n",
    "        self.length = len(self.img_root_names)\n",
    "        \n",
    "        #generate file names for spectral cubes and jpg images\n",
    "        self.rgb_image_files = []\n",
    "        self.hsi_image_files = []\n",
    "        for img_root_name in self.img_root_names:\n",
    "            self.rgb_image_files.append(img_root_name+\"_RealWorld.jpg\")\n",
    "            self.hsi_image_files.append(img_root_name+\".mat\")\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        rgb_img_path = self.rgb_path + self.rgb_image_files[index]\n",
    "        hsi_img_path = self.hsi_path + self.hsi_image_files[index]\n",
    "        img_root_name = self.img_root_names[index]\n",
    "\n",
    "        hsi_img = io.loadmat(hsi_img_path)[\"cube\"]\n",
    "        rgb_img = cv2.imread(rgb_img_path,-1)\n",
    "\n",
    "        rgb_img = rgb_img.astype(np.float64)/255.0 #normalisation to [0,1]\n",
    "\n",
    "        #crop a patch of the scene\n",
    "        h,w = rgb_img.shape[:2]\n",
    "        if (h>self.input_image_size) or (w>self.input_image_size):\n",
    "            rand_h = random.randint(0,h-self.input_image_size)\n",
    "            rand_w = random.randint(0,w-self.input_image_size)\n",
    "\n",
    "            rgb_img = rgb_img[rand_h:rand_h+self.input_image_size, rand_w:rand_w+self.input_image_size, :] \n",
    "            hsi_img = hsi_img[rand_h:rand_h+self.input_image_size, rand_w:rand_w+self.input_image_size, :] \n",
    "            \n",
    "        rgb_img = torch.from_numpy(rgb_img.astype(np.float32).transpose(2, 0, 1)).contiguous() #to tensor\n",
    "        hsi_img = torch.from_numpy(hsi_img.astype(np.float32).transpose(2, 0, 1)).contiguous()\n",
    "            \n",
    "        return rgb_img, hsi_img, img_root_name\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in training set 450\n"
     ]
    }
   ],
   "source": [
    "TrainDataset = HSI_Dataset_Train(\"Data/train/\",[256,256,3],data_transforms=None)\n",
    "print(f\"Number of images in training set {len(TrainDataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TrainDataset, batch_size = 8, shuffle = True, num_workers = 0, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)\n",
    "first_batch = train_iter.next()\n",
    "rgb_images,hsi_images,  img_names = first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hsi_images))\n",
    "print(hsi_images[0].shape)\n",
    "\n",
    "print(len(rgb_images))\n",
    "print(rgb_images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customised Layers for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixel Unshuffle layer - does opposite to what torch.nn.Functional.pixelshuffle does\n",
    "\n",
    "def pixel_unshuffle(input, downscale_factor):\n",
    "    c = input.shape[1]\n",
    "    kernel = torch.zeros(size = [downscale_factor * downscale_factor * c, 1, downscale_factor, downscale_factor],\n",
    "                        device = input.device)\n",
    "    for y in range(downscale_factor):\n",
    "        for x in range(downscale_factor):\n",
    "            kernel[x + y * downscale_factor::downscale_factor * downscale_factor, 0, y, x] = 1\n",
    "    return F.conv2d(input, kernel, stride = downscale_factor, groups = c)\n",
    "\n",
    "\n",
    "class PixelUnShuffle(nn.Module):\n",
    "    def __init__(self, downscale_factor):\n",
    "        super(PixelUnShuffle, self).__init__()\n",
    "        self.downscale_factor = downscale_factor\n",
    "\n",
    "    def forward(self, input):\n",
    "        return pixel_unshuffle(input, self.downscale_factor)\n",
    "# Conv2d Block\n",
    "class Conv2dLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False):\n",
    "        super(Conv2dLayer, self).__init__()\n",
    "        # Initialize the padding scheme\n",
    "        if pad_type == 'reflect':\n",
    "            self.pad = nn.ReflectionPad2d(padding)\n",
    "        elif pad_type == 'replicate':\n",
    "            self.pad = nn.ReplicationPad2d(padding)\n",
    "        elif pad_type == 'zero':\n",
    "            self.pad = nn.ZeroPad2d(padding)\n",
    "        else:\n",
    "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
    "        \n",
    "        # Initialize the normalization type\n",
    "        if norm == 'bn':\n",
    "            self.norm = nn.BatchNorm2d(out_channels)\n",
    "        elif norm == 'in':\n",
    "            self.norm = nn.InstanceNorm2d(out_channels)\n",
    "        elif norm == 'ln':\n",
    "            self.norm = LayerNorm(out_channels)\n",
    "        elif norm == 'none':\n",
    "            self.norm = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported normalization: {}\".format(norm)\n",
    "        \n",
    "        # Initialize the activation funtion\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU(inplace = True)\n",
    "        elif activation == 'lrelu':\n",
    "            self.activation = nn.LeakyReLU(0.2, inplace = True)\n",
    "        elif activation == 'prelu':\n",
    "            self.activation = nn.PReLU()\n",
    "        elif activation == 'selu':\n",
    "            self.activation = nn.SELU(inplace = True)\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'none':\n",
    "            self.activation = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
    "\n",
    "        # Initialize the convolution layers\n",
    "        if sn:\n",
    "            self.conv2d = SpectralNorm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation))\n",
    "        else:\n",
    "            self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding = 0, dilation = dilation)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        x = self.conv2d(x)\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class TransposeConv2dLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False, scale_factor = 2):\n",
    "        super(TransposeConv2dLayer, self).__init__()\n",
    "        # Initialize the conv scheme\n",
    "        self.scale_factor = scale_factor\n",
    "        self.conv2d = Conv2dLayer(in_channels, out_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor = self.scale_factor, mode = 'nearest')\n",
    "        x = self.conv2d(x)\n",
    "        return x\n",
    "\n",
    "class ResConv2dLayer(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size = 3, stride = 1, padding = 1, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False, scale_factor = 2):\n",
    "        super(ResConv2dLayer, self).__init__()\n",
    "        # Initialize the conv scheme\n",
    "        self.conv2d = nn.Sequential(\n",
    "            Conv2dLayer(in_channels, in_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn),\n",
    "            Conv2dLayer(in_channels, in_channels, kernel_size, stride, padding, dilation, pad_type, activation = 'none', norm = norm, sn = sn)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv2d(x)\n",
    "        out = 0.1 * out + residual\n",
    "        return out\n",
    "\n",
    "class DenseConv2dLayer_5C(nn.Module):\n",
    "    def __init__(self, in_channels, latent_channels, kernel_size = 3, stride = 1, padding = 1, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False):\n",
    "        super(DenseConv2dLayer_5C, self).__init__()\n",
    "        # dense convolutions\n",
    "        self.conv1 = Conv2dLayer(in_channels, latent_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "        self.conv2 = Conv2dLayer(in_channels + latent_channels, latent_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "        self.conv3 = Conv2dLayer(in_channels + latent_channels * 2, latent_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "        self.conv4 = Conv2dLayer(in_channels + latent_channels * 3, latent_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "        self.conv5 = Conv2dLayer(in_channels + latent_channels * 4, in_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(torch.cat((x, x1), 1))\n",
    "        x3 = self.conv3(torch.cat((x, x1, x2), 1))\n",
    "        x4 = self.conv4(torch.cat((x, x1, x2, x3), 1))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        return x5\n",
    "        \n",
    "class ResidualDenseBlock_5C(nn.Module):\n",
    "    def __init__(self, in_channels, latent_channels, kernel_size = 3, stride = 1, padding = 1, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False):\n",
    "        super(ResidualDenseBlock_5C, self).__init__()\n",
    "        # dense convolutions\n",
    "        self.conv1 = Conv2dLayer(in_channels, latent_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "        self.conv2 = Conv2dLayer(in_channels + latent_channels, latent_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "        self.conv3 = Conv2dLayer(in_channels + latent_channels * 2, latent_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "        self.conv4 = Conv2dLayer(in_channels + latent_channels * 3, latent_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "        self.conv5 = Conv2dLayer(in_channels + latent_channels * 4, in_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(torch.cat((x, x1), 1))\n",
    "        x3 = self.conv3(torch.cat((x, x1, x2), 1))\n",
    "        x4 = self.conv4(torch.cat((x, x1, x2, x3), 1))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        x5 = 0.1 * x5 + residual\n",
    "        return x5\n",
    "\n",
    "# Layer Norm\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps = 1e-8, affine = True):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = Parameter(torch.Tensor(num_features).uniform_())\n",
    "            self.beta = Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer norm\n",
    "        shape = [-1] + [1] * (x.dim() - 1)                                  # for 4d input: [-1, 1, 1, 1]\n",
    "        if x.size(0) == 1:\n",
    "            # These two lines run much faster in pytorch 0.4 than the two lines listed below.\n",
    "            mean = x.view(-1).mean().view(*shape)\n",
    "            std = x.view(-1).std().view(*shape)\n",
    "        else:\n",
    "            mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "            std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "        # if it is learnable\n",
    "        if self.affine:\n",
    "            shape = [1, -1] + [1] * (x.dim() - 2)                          # for 4d input: [1, -1, 1, 1]\n",
    "            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
    "        return x\n",
    "\n",
    "# spectral norm\n",
    "def l2normalize(v, eps = 1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name = 'weight', power_iterations = 1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
    "\n",
    "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)\n",
    "\n",
    "#non local block\n",
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer for Feature Map dimension\"\"\"\n",
    "    def __init__(self, in_dim, latent_dim = 8):\n",
    "        super(Self_Attn, self).__init__()\n",
    "        self.channel_in = in_dim\n",
    "        self.channel_latent = in_dim // latent_dim\n",
    "        self.query_conv = nn.Conv2d(in_channels = in_dim, out_channels = in_dim // latent_dim, kernel_size = 1)\n",
    "        self.key_conv = nn.Conv2d(in_channels = in_dim, out_channels = in_dim // latent_dim, kernel_size = 1)\n",
    "        self.value_conv = nn.Conv2d(in_channels = in_dim, out_channels = in_dim, kernel_size = 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax  = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        batchsize, C, height, width = x.size()\n",
    "        # proj_query: reshape to B x N x c, N = H x W\n",
    "        proj_query  = self.query_conv(x).view(batchsize, -1, height * width).permute(0, 2, 1)\n",
    "        # proj_query: reshape to B x c x N, N = H x W\n",
    "        proj_key =  self.key_conv(x).view(batchsize, -1, height * width)\n",
    "        # transpose check, energy: B x N x N, N = H x W\n",
    "        energy =  torch.bmm(proj_query, proj_key)\n",
    "        # attention: B x N x N, N = H x W\n",
    "        attention = self.softmax(energy)\n",
    "        # proj_value is normal convolution, B x C x N\n",
    "        proj_value = self.value_conv(x).view(batchsize, -1, height * width)\n",
    "        # out: B x C x N\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batchsize, C, height, width)\n",
    "        \n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "#Global Block\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction = 16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias = False),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(channel // reduction, channel // reduction, bias = False),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(channel // reduction, channel, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class GlobalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size, stride = 1, padding = 0, dilation = 1, pad_type = 'zero', activation = 'lrelu', norm = 'none', sn = False, reduction = 8):\n",
    "        super(GlobalBlock, self).__init__()\n",
    "        self.conv1 = Conv2dLayer(in_channels, in_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "        self.conv2 = Conv2dLayer(in_channels, in_channels, kernel_size, stride, padding, dilation, pad_type, activation, norm, sn)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias = False),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(in_channels // reduction, in_channels // reduction, bias = False),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # residual\n",
    "        residual = x\n",
    "        # Sequeeze-and-Excitation(SE)\n",
    "        b, c, _, _ = x.size()\n",
    "        x = self.conv1(x)\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        y = x * y.expand_as(x)\n",
    "        y = self.conv2(y)\n",
    "        # addition\n",
    "        out = 0.1 * y + residual\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Designing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB2HS(nn.Module):\n",
    "    def __init__(self, in_channels,out_channels,start_channels,activ,norm,pad):\n",
    "        super(RGB2HS, self).__init__()\n",
    "        # Top subnetwork, K = 3\n",
    "        self.top1 = Conv2dLayer(in_channels * (4 ** 3), start_channels * (2 ** 3), 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.top21 = ResidualDenseBlock_5C(start_channels * (2 ** 3), start_channels * (2 ** 2), 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.top22 = GlobalBlock(start_channels * (2 ** 3), 3, 1, 1, pad_type = pad, activation = activ, norm = norm, sn = False, reduction = 4)\n",
    "        self.top3 = Conv2dLayer(start_channels * (2 ** 3), start_channels * (2 ** 3), 1, 1, 0, pad_type = pad, activation = activ, norm = norm)\n",
    "        # Middle subnetwork, K = 2\n",
    "        self.mid1 = Conv2dLayer(in_channels * (4 ** 2), start_channels * (2 ** 2), 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.mid2 = Conv2dLayer(int(start_channels * (2 ** 2 + 2 ** 3 / 4)), start_channels * (2 ** 2), 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.mid31 = ResidualDenseBlock_5C(start_channels * (2 ** 2), start_channels * (2 ** 1), 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.mid32 = GlobalBlock(start_channels * (2 ** 2), 3, 1, 1, pad_type = pad, activation = activ, norm = norm, sn = False, reduction = 4)\n",
    "        self.mid4 = Conv2dLayer(start_channels * (2 ** 2), start_channels * (2 ** 2), 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        # Bottom subnetwork, K = 1\n",
    "        self.bot1 = Conv2dLayer(in_channels * (4 ** 1), start_channels * (2 ** 1), 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.bot2 = Conv2dLayer(int(start_channels * (2 ** 1 + 2 ** 2 / 4)), start_channels * (2 ** 1), 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.bot31 = ResidualDenseBlock_5C(start_channels * (2 ** 1), start_channels * (2 ** 0), 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.bot32 = ResidualDenseBlock_5C(start_channels * (2 ** 1), start_channels * (2 ** 0), 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.bot33 = GlobalBlock(start_channels * (2 ** 1), 3, 1, 1, pad_type = pad, activation = activ, norm = norm, sn = False, reduction = 4)\n",
    "        self.bot4 = Conv2dLayer(start_channels * (2 ** 1), start_channels * (2 ** 1), 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        # Mainstream\n",
    "        self.main1 = Conv2dLayer(in_channels, start_channels, 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.main2 = Conv2dLayer(int(start_channels * (2 ** 0 + 2 ** 1 / 4)), start_channels, 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.main31 = ResidualDenseBlock_5C(start_channels, start_channels // 2, 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.main32 = ResidualDenseBlock_5C(start_channels, start_channels // 2, 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.main33 = ResidualDenseBlock_5C(start_channels, start_channels // 2, 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.main34 = ResidualDenseBlock_5C(start_channels, start_channels // 2, 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "        self.main35 = GlobalBlock(start_channels, 3, 1, 1, pad_type = pad, activation = activ, norm = norm, sn = False, reduction = 4)\n",
    "        self.main4 = Conv2dLayer(start_channels, out_channels, 3, 1, 1, pad_type = pad, activation = activ, norm = norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PixelUnShuffle                                        input: batch * 3 * 256 * 256\n",
    "        x1 = pixel_unshuffle(x, 2)               # out: batch * 12 * 128 * 128\n",
    "        x2 = pixel_unshuffle(x, 4)               # out: batch * 48 * 64 * 64\n",
    "        x3 = pixel_unshuffle(x, 8)               # out: batch * 192 * 32 * 32\n",
    "        # Top subnetwork                                        suppose the start_channels = 32\n",
    "        x3 = self.top1(x3)                                      # out: batch * 256 * 32 * 32\n",
    "        x3 = self.top21(x3)                                     # out: batch * 256 * 32 * 32\n",
    "        x3 = self.top22(x3)                                     # out: batch * 256 * 32 * 32\n",
    "        x3 = self.top3(x3)                                      # out: batch * 256 * 32 * 32\n",
    "        x3 = F.pixel_shuffle(x3, 2)                             # out: batch * 64 * 64 * 64, ready to be concatenated\n",
    "        # Middle subnetwork\n",
    "        x2 = self.mid1(x2)                                      # out: batch * 128 * 64 * 64\n",
    "        x2 = torch.cat((x2, x3), 1)                             # out: batch * (128 + 64) * 64 * 64\n",
    "        x2 = self.mid2(x2)                                      # out: batch * 128 * 64 * 64\n",
    "        x2 = self.mid31(x2)                                     # out: batch * 128 * 64 * 64\n",
    "        x2 = self.mid32(x2)                                     # out: batch * 128 * 64 * 64\n",
    "        x2 = self.mid4(x2)                                      # out: batch * 128 * 64 * 64\n",
    "        x2 = F.pixel_shuffle(x2, 2)                             # out: batch * 32 * 128 * 128, ready to be concatenated\n",
    "        # Bottom subnetwork\n",
    "        x1 = self.bot1(x1)                                      # out: batch * 64 * 128 * 128\n",
    "        x1 = torch.cat((x1, x2), 1)                             # out: batch * (64 + 32) * 128 * 128\n",
    "        x1 = self.bot2(x1)                                      # out: batch * 64 * 128 * 128\n",
    "        x1 = self.bot31(x1)                                     # out: batch * 64 * 128 * 128\n",
    "        x1 = self.bot32(x1)                                     # out: batch * 64 * 128 * 128\n",
    "        x1 = self.bot33(x1)                                     # out: batch * 64 * 128 * 128\n",
    "        x1 = self.bot4(x1)                                      # out: batch * 64 * 128 * 128\n",
    "        x1 = F.pixel_shuffle(x1, 2)                             # out: batch * 16 * 256 * 256, ready to be concatenated\n",
    "        # U-Net generator with skip connections from encoder to decoder\n",
    "        x = self.main1(x)                                       # out: batch * 32 * 256 * 256\n",
    "        x = torch.cat((x, x1), 1)                               # out: batch * (32 + 16) * 256 * 256\n",
    "        x = self.main2(x)                                       # out: batch * 32 * 256 * 256\n",
    "        x = self.main31(x)                                      # out: batch * 32 * 256 * 256\n",
    "        x = self.main32(x)                                      # out: batch * 32 * 256 * 256\n",
    "        x = self.main33(x)                                      # out: batch * 32 * 256 * 256\n",
    "        x = self.main34(x)                                      # out: batch * 32 * 256 * 256\n",
    "        x = self.main35(x)                                      # out: batch * 32 * 256 * 256\n",
    "        x = self.main4(x)                                       # out: batch * 3 * 256 * 256\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RGB2HS(3,31,64,'lrelu','none','reflect')\n",
    "# model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model,input_size=(3,256,256)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model,input_size=(3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 0 \n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# CHECKPOINT_FOLDER = \"Checkpoints/\"\n",
    "# CHECKPOINT_FILE = \"EfNetB3_v1.pth.tar\"\n",
    "\n",
    "PIN_MEMORY = True\n",
    "# SAVE_MODEL = True\n",
    "# LOAD_MODEL = True\n",
    "\n",
    "INPUT_IMAGE_WIDTH = [256]\n",
    "INPUT_IMAGE_HEIGHT = [256]\n",
    "INPUT_IMAGE_SIZE = INPUT_IMAGE_WIDTH + INPUT_IMAGE_HEIGHT\n",
    "INPUT_IMAGE_CHANNELS = [3]\n",
    "INPUT_IMAGE_SHAPE = INPUT_IMAGE_SIZE + INPUT_IMAGE_CHANNELS\n",
    "\n",
    "TRAIN_DATA_DIR = \"Data/train/\"\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_B1 = 0.5\n",
    "WEIGHT_B2 = 0.999\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "CHECKPOINT_FOLDER = \"Checkpoints/\"\n",
    "CHECKPOINT_FILE = \"SpecReconV1OnlineData_full.pth\" #.pth\n",
    "\n",
    "writer = SummaryWriter(\"runs/SpecRecon_v1_online_data_full dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in training set 450\n"
     ]
    }
   ],
   "source": [
    "model = RGB2HS(3,31,64,'lrelu','none','reflect')\n",
    "weights_init(model, init_type = 'xavier', init_gain = 0.02)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "TrainDataset = HSI_Dataset_Train(TRAIN_DATA_DIR,INPUT_IMAGE_SHAPE,data_transforms=None)\n",
    "print(f\"Number of images in training set {len(TrainDataset)}\")\n",
    "\n",
    "train_loader = DataLoader(TrainDataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = NUM_WORKERS, pin_memory = PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss FUnction, Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.L1Loss().cuda()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, betas = (WEIGHT_B1, WEIGHT_B2), weight_decay = WEIGHT_DECAY)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "lr_scheduler1 = lr_scheduler.StepLR(optimiser,step_size=250,gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,dataloader,optimiser,scheduler,loss_func,scaler,epoch_num):\n",
    "    total_loss = 0\n",
    "    dataiter = tqdm(dataloader)\n",
    "    for i,(rgb_img,hsi_img,img_name) in enumerate(dataiter):\n",
    "        rgb_img = rgb_img.to(DEVICE)\n",
    "        hsi_img = hsi_img.to(DEVICE)\n",
    "\n",
    "        #forward pass\n",
    "        with torch.cuda.amp.autocast():\n",
    "            recons_img = model(rgb_img)\n",
    "            batch_loss = loss_func(recons_img,hsi_img)\n",
    "            total_loss += batch_loss.item()\n",
    "            \n",
    "        #backward step\n",
    "        optimiser.zero_grad()\n",
    "        scaler.scale(batch_loss).backward()\n",
    "        scaler.step(optimiser)\n",
    "        scaler.update()\n",
    "        lr_scheduler1.step()\n",
    "        dataiter.set_postfix(loss=batch_loss.item())\n",
    "        \n",
    "    epoch_loss = total_loss/len(dataloader)\n",
    "    writer.add_scalar('MAE_loss',epoch_loss,global_step=epoch_num)\n",
    "    writer.close()\n",
    "#     if epoch_num%100==0:\n",
    "    print(f\"Epoch {epoch_num}, Loss {epoch_loss}\")\n",
    "    return epoch_loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.33it/s, loss=0.0815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.06556085799717241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.32it/s, loss=0.0425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.041837270822789936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.33it/s, loss=0.0349] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss 0.04100427851287855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:38<00:00,  2.29it/s, loss=0.0247] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss 0.041213166308071876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.33it/s, loss=0.0244] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss 0.04084135151778658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.33it/s, loss=0.0456] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss 0.04076602334777514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0204] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss 0.04081608824431896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.35it/s, loss=0.0671] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss 0.04092896956536505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.33it/s, loss=0.0359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss 0.04034722871664498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.33it/s, loss=0.033]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss 0.04055109910459982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.37it/s, loss=0.0796] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss 0.04042940511471695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.38it/s, loss=0.0473] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss 0.04074585429496235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.38it/s, loss=0.0234] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss 0.04108961870893836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0786] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss 0.040624146991305884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.38it/s, loss=0.0383] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss 0.041570554218358466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0249] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss 0.04059853114601639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0176] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss 0.04060666109952662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss 0.04095387700945139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:38<00:00,  2.27it/s, loss=0.0433] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss 0.04098437938839197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.30it/s, loss=0.0404] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss 0.040452718796829386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:38<00:00,  2.28it/s, loss=0.0115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss 0.040803615454998284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.30it/s, loss=0.0186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss 0.04079579018470314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.32it/s, loss=0.0348] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss 0.040770741740448604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.31it/s, loss=0.0419] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss 0.040911201474567255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:38<00:00,  2.29it/s, loss=0.0533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss 0.04020418982124991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.34it/s, loss=0.0294] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss 0.04170353556258811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.37it/s, loss=0.032]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss 0.040800969373020864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.37it/s, loss=0.0182] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss 0.040504590498490464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.37it/s, loss=0.0361] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss 0.040436577366458046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0314] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss 0.04035615060064528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.30it/s, loss=0.0376] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss 0.04030147278267476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.33it/s, loss=0.0522] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss 0.04072173051329123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.34it/s, loss=0.0335] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss 0.041158003146863645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.37it/s, loss=0.0275] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss 0.0410289044967956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.37it/s, loss=0.048]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Loss 0.04078147942200303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.37it/s, loss=0.0559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Loss 0.04113164725816912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.00873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss 0.041369544424944456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.38it/s, loss=0.0398] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss 0.04128971901618772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.37it/s, loss=0.0808] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss 0.04125530114190446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.35it/s, loss=0.0555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Loss 0.04069312654021714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.32it/s, loss=0.0872] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss 0.04061228990347849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:39<00:00,  2.27it/s, loss=0.041] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Loss 0.03992189841551913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:39<00:00,  2.27it/s, loss=0.0219] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Loss 0.04087017623707652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:38<00:00,  2.29it/s, loss=0.0879] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Loss 0.04098172065284517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:38<00:00,  2.29it/s, loss=0.0302] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Loss 0.04104617925567759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.32it/s, loss=0.0246] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Loss 0.04063085508843263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:39<00:00,  2.26it/s, loss=0.0436] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Loss 0.040229817651626136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.34it/s, loss=0.0775] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Loss 0.040522443352060185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.37it/s, loss=0.0381] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Loss 0.04091128734250864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.35it/s, loss=0.0145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Loss 0.04157537032539646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0339] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss 0.04039242411653201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.31it/s, loss=0.0311] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, Loss 0.040871697035100726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.31it/s, loss=0.00819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, Loss 0.04160178371808595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:38<00:00,  2.29it/s, loss=0.0972] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, Loss 0.041340159032907754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.32it/s, loss=0.0252] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, Loss 0.03990380442183879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.33it/s, loss=0.0677] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55, Loss 0.04152699755297767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.34it/s, loss=0.0129] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, Loss 0.040911396640456385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.34it/s, loss=0.00571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, Loss 0.04043928465288547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.34it/s, loss=0.038] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, Loss 0.040724327671859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.30it/s, loss=0.0572] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, Loss 0.04090990070874492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:39<00:00,  2.27it/s, loss=0.0652] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Loss 0.041011621811323694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:42<00:00,  2.19it/s, loss=0.0446] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, Loss 0.040935354895061914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:39<00:00,  2.26it/s, loss=0.0332] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, Loss 0.04102139346715477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.34it/s, loss=0.0367] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, Loss 0.040229693858159915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0514] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, Loss 0.04137041169322199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.35it/s, loss=0.0309] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, Loss 0.04040788381256991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0408] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, Loss 0.04075706483796239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.30it/s, loss=0.016]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, Loss 0.04115014864131808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.33it/s, loss=0.0592] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, Loss 0.041055587397681344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.35it/s, loss=0.0136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, Loss 0.040857503335509034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:38<00:00,  2.29it/s, loss=0.028]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, Loss 0.040883250395870874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:38<00:00,  2.27it/s, loss=0.0266] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, Loss 0.04053987538028094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:39<00:00,  2.26it/s, loss=0.0233] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, Loss 0.040859780663417444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.31it/s, loss=0.0438] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, Loss 0.040186455465025375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:38<00:00,  2.28it/s, loss=0.0559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, Loss 0.04041419621970919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.32it/s, loss=0.0502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Loss 0.040553990993648766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.34it/s, loss=0.027]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, Loss 0.04087033724205361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.33it/s, loss=0.0574] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, Loss 0.040095378371576465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.34it/s, loss=0.0266] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, Loss 0.03994101274137696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.32it/s, loss=0.0297] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, Loss 0.04143315695019232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:37<00:00,  2.31it/s, loss=0.0472] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Loss 0.0405866617233389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:38<00:00,  2.29it/s, loss=0.0434] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, Loss 0.04076615801701943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.34it/s, loss=0.0459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Loss 0.04096736317293512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.35it/s, loss=0.0286] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, Loss 0.04142142458508412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:36<00:00,  2.32it/s, loss=0.0844] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, Loss 0.041288666762411594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0155] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, Loss 0.04059868760406971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, Loss 0.041113055418762895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.35it/s, loss=0.0178] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, Loss 0.04037062184471223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.37it/s, loss=0.0157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88, Loss 0.041013087814466824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.35it/s, loss=0.057] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, Loss 0.04048398819234636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0122] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Loss 0.040956406945155725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.35it/s, loss=0.0354] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, Loss 0.04040572730617391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, Loss 0.04073727660915918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0557] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, Loss 0.04132699180808332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.37it/s, loss=0.0631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, Loss 0.04061649943391482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:35<00:00,  2.36it/s, loss=0.0317] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Loss 0.041056230761524704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.37it/s, loss=0.0506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, Loss 0.04099229085155659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.37it/s, loss=0.0495] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, Loss 0.041034724277754626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.37it/s, loss=0.0273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, Loss 0.04173011793237594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [01:34<00:00,  2.37it/s, loss=0.0244] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, Loss 0.04068570215668943\n",
      "Best Epoch : 54, Best Epoch Loss : 0.03990380442183879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# trainer = tqdm(range(NUM_EPOCHS))\n",
    "best_loss = 10000\n",
    "best_epoch = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epochloss = train_epoch(model,train_loader,optimiser,lr_scheduler1,loss_func,scaler,epoch)\n",
    "    if epochloss<best_loss :\n",
    "        best_loss = epochloss\n",
    "        best_epoch = epoch\n",
    "        checkpoint_stuff = {\"state_dict\":model.state_dict() ,\"optimiser\":optimiser.state_dict()}\n",
    "        best_epoch = epoch\n",
    "#         print(f\"Checkpoint Saved at {best_epoch}\")\n",
    "        save_checkpoint(checkpoint_stuff)\n",
    "    #trainer.set_postfix(loss=epochloss)\n",
    "print(f\"Best Epoch : {best_epoch}, Best Epoch Loss : {best_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best model\n",
    "model = RGB2HS(3,31,64,'lrelu','none','reflect')\n",
    "weights_init(model, init_type = 'xavier', init_gain = 0.02)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, betas = (WEIGHT_B1, WEIGHT_B2), weight_decay = WEIGHT_DECAY)\n",
    "load_checkpoint(CHECKPOINT_FOLDER+CHECKPOINT_FILE,model,optimiser,LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainDataset = HSI_Dataset_Train(\"Dataset/validation-trial-norway-2/\",\"_flash_img.jpg\",\"_spectra_2.mat\",[32,32,3],data_transforms=None)\n",
    "print(f\"Number of images in training set {len(TrainDataset)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "83bec4cd18966703d39e3fd8209371b7574445e5ac2dfe27a80cb33e9b531167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
